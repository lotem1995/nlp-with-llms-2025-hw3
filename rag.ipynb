{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7aae3a9",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "Implement a base RAG module in DSPy. \n",
    "Given a question, retrieve the top-k documents in a list of HTML documents, then pass them as context to an LLM.\n",
    "\n",
    "Refer to https://dspy.ai/tutorials/rag/. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edec48e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T11:35:31.165451Z",
     "start_time": "2025-09-06T11:35:23.540639Z"
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load an extremely efficient local model for retrieval\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "\n",
    "# Create an embedder using the model's encode method\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "# Traverse a directory and read html files - extract text from the html files\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "def read_html_files(directory):\n",
    "    texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".html\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file, 'html.parser')\n",
    "                texts.append(soup.get_text())\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f634c8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T11:35:34.243578Z",
     "start_time": "2025-09-06T11:35:31.170252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 406 documents. Will encode them below.\n"
     ]
    }
   ],
   "source": [
    "corpus = read_html_files(\"../PragmatiCQA-sources/The Legend of Zelda\")\n",
    "print(f\"Loaded {len(corpus)} documents. Will encode them below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0051d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T11:35:34.318440Z",
     "start_time": "2025-09-06T11:35:34.295096Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters for the retriever\n",
    "max_characters = 10000  # for truncating >99th percentile of documents\n",
    "topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
    "\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b129af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T11:35:34.343912Z",
     "start_time": "2025-09-06T11:35:34.341798Z"
    }
   },
   "outputs": [],
   "source": [
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini',api_key=\"\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06d8027",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T11:35:34.390043Z",
     "start_time": "2025-09-06T11:35:34.386485Z"
    }
   },
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = search(question).passages\n",
    "        return self.respond(context=context, question=question)\n",
    "    \n",
    "rag = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499af707",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T11:35:34.540748Z",
     "start_time": "2025-09-06T11:35:34.433454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main plot of The Legend of Zelda revolves around a young hero named Link who must save the kingdom of Hyrule from the evil Ganon, the Prince of Darkness. Ganon steals the Triforce of Power and seeks to conquer Hyrule, prompting Princess Zelda to break the Triforce of Wisdom into eight fragments and hide them across the land. Zelda sends her nursemaid, Impa, to find a brave warrior to stop Ganon. Link embarks on a quest to collect the fragments, explore dungeons, battle enemies, and ultimately confront Ganon to rescue Princess Zelda and restore peace to Hyrule.\n"
     ]
    }
   ],
   "source": [
    "answer = rag(question=\"What is the main plot of The Legend of Zelda?\")  # Example query\n",
    "\n",
    "print(answer.response)  # Print the response from the RAG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98e19c23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T11:35:34.653729Z",
     "start_time": "2025-09-06T11:35:34.548103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Legend of Zelda was first released in 1986.\n"
     ]
    }
   ],
   "source": [
    "q = 'What year did the Legend of Zelda come out?' \n",
    "\n",
    "print(rag(question=q).response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499264ba056e3f27",
   "metadata": {},
   "source": [
    "# Part 1 — Traditional QA Baseline (SemanticF1)\n",
    "\n",
    "## Setup (what we evaluated)\n",
    "\n",
    "* **Model:** `distilbert-base-cased-distilled-squad` (extractive QA via Hugging Face `pipeline`). We keep default QA pipeline chunking, where long contexts are split with a stride and `max_seq_len≈384` unless overridden.\n",
    "* **Data slice:** the **first question of each conversation** in `val.jsonl` from **PragmatiCQA**. The dataset includes both **literal** and **pragmatic** annotations for answers, designed to test whether systems handle implied intent, not just verbatim spans.\n",
    "* **Configs compared:**\n",
    "\n",
    "  1. **Literal** (gold literal spans → QA),\n",
    "  2. **Pragmatic** (gold pragmatic spans → QA),\n",
    "  3. **Retrieved** (top-k passages retrieved from HTML sources → QA).\n",
    "* **Metric:** **SemanticF1** (DSPy), computed with an LLM that compares candidate vs. gold answers using key-idea overlap. We use the **batched** API so each item is a `dspy.Example` with `example` (gold) and `pred` (system) inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2057fe189087ff06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T11:35:35.345918Z",
     "start_time": "2025-09-06T11:35:34.658144Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# === Part 1 — setup (baseline QA + data) ===\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "BASE = Path.cwd().parent  # adjust if your notebook is elsewhere\n",
    "DATA_VAL = BASE / \"PragmatiCQA\" / \"data\" / \"val.jsonl\"\n",
    "SOURCES = BASE / \"PragmatiCQA-sources\"\n",
    "\n",
    "# Extractive QA baseline\n",
    "qa = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", device=-1)\n",
    "\n",
    "# First question per validation conversation (≈179)\n",
    "def load_val_first_questions(jsonl_path: Path):\n",
    "    rows = []\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rec = json.loads(line)\n",
    "            q0 = rec[\"qas\"][0]\n",
    "            literal = \" \".join(x.get(\"text\",\"\") for x in q0.get(\"a_meta\",{}).get(\"literal_obj\", [])).strip()\n",
    "            pragmatic = \" \".join(x.get(\"text\",\"\") for x in q0.get(\"a_meta\",{}).get(\"pragmatic_obj\", [])).strip()\n",
    "            rows.append({\n",
    "                \"topic\": rec[\"topic\"],\n",
    "                \"question\": q0[\"q\"],\n",
    "                \"gold_answer\": q0[\"a\"],\n",
    "                \"literal_ctx\": literal,\n",
    "                \"pragmatic_ctx\": pragmatic\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3473949d76dd053b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T11:36:30.069381Z",
     "start_time": "2025-09-06T11:36:30.064032Z"
    }
   },
   "outputs": [],
   "source": [
    "max_characters = 10000\n",
    "\n",
    "def read_topic_html_texts(topic_dir: Path):\n",
    "    texts = []\n",
    "    if not topic_dir.exists():\n",
    "        return texts\n",
    "    for fname in os.listdir(topic_dir):\n",
    "        if fname.endswith(\".html\"):\n",
    "            try:\n",
    "                with open(topic_dir / fname, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    soup = BeautifulSoup(f, \"html.parser\")\n",
    "                    txt = soup.get_text(\" \", strip=True)\n",
    "                    if txt:\n",
    "                        texts.append(txt[:max_characters])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return texts\n",
    "\n",
    "RETRIEVERS = {}\n",
    "\n",
    "def get_retriever_for_topic(topic: str, k: int = 6):\n",
    "    # cache even the \"None\" so we don't retry building on empty topics each time\n",
    "    if topic in RETRIEVERS:\n",
    "        return RETRIEVERS[topic]\n",
    "\n",
    "    corpus = read_topic_html_texts(SOURCES / topic)\n",
    "    if not corpus:\n",
    "        RETRIEVERS[topic] = None\n",
    "        return None\n",
    "\n",
    "    # ensure numpy output from SentenceTransformer.encode\n",
    "    global embedder\n",
    "    try:\n",
    "        _ = embedder([\"probe\"])  # should return (1, D) array\n",
    "    except TypeError:\n",
    "        # If your embedder needs explicit numpy conversion\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"gpu\")\n",
    "        embedder = dspy.Embedder(lambda xs: model.encode(xs, convert_to_numpy=True))\n",
    "\n",
    "    RETRIEVERS[topic] = dspy.retrievers.Embeddings(\n",
    "        embedder=embedder, corpus=corpus, k=k, normalize=True\n",
    "    )\n",
    "    return RETRIEVERS[topic]\n",
    "\n",
    "def retrieve_context(topic: str, question: str, k: int = 6) -> str:\n",
    "    retr = get_retriever_for_topic(topic, k=k)\n",
    "    if retr is None:\n",
    "        return \"\"   # <- gracefully skip empty topics\n",
    "    return \" \".join(retr(question).passages)\n",
    "\n",
    "\n",
    "def build_sf1_batch(preds, golds, questions):\n",
    "    batch = []\n",
    "    for p, g, q in zip(preds, golds, questions):\n",
    "        gold_ex = dspy.Example(question=q, response=g)\n",
    "        pred_ex = dspy.Prediction(response=p or \"\")\n",
    "        item = dspy.Example(example=gold_ex, pred=pred_ex).with_inputs(\"example\", \"pred\")\n",
    "        batch.append(item)\n",
    "    return batch\n",
    "def lexical_f1(pred: str, gold: str):\n",
    "    p, g = _normalize(pred), _normalize(gold)\n",
    "    if not p or not g:\n",
    "        return {\"precision\":0.0, \"recall\":0.0, \"f1\":0.0}\n",
    "    from collections import Counter\n",
    "    pc, gc = Counter(p), Counter(g)\n",
    "    common = sum((pc & gc).values())\n",
    "    prec = common / sum(pc.values())\n",
    "    rec  = common / sum(gc.values())\n",
    "    f1   = 0.0 if prec+rec == 0 else 2*prec*rec/(prec+rec)\n",
    "    return {\"precision\":prec, \"recall\":rec, \"f1\":f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50bc181ce7353167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T11:44:56.048231Z",
     "start_time": "2025-09-06T11:36:31.911826Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Part 1: 100%|██████████| 179/179 [08:24<00:00,  2.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# === Part 1 — run the three configurations and evaluate ===\n",
    "val_df = load_val_first_questions(DATA_VAL)\n",
    "\n",
    "rows = []\n",
    "for _, r in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Evaluating Part 1\"):\n",
    "    topic, q, gold = r[\"topic\"], r[\"question\"], r[\"gold_answer\"]\n",
    "\n",
    "    # 1) literal\n",
    "    pred_lit  = qa(question=q, context=r[\"literal_ctx\"]).get(\"answer\",\"\") if r[\"literal_ctx\"] else \"\"\n",
    "\n",
    "    # 2) pragmatic\n",
    "    pred_prag = qa(question=q, context=r[\"pragmatic_ctx\"]).get(\"answer\",\"\") if r[\"pragmatic_ctx\"] else \"\"\n",
    "\n",
    "    # 3) retrieved (uses your DSPy embeddings retriever per topic)\n",
    "    ctx_ret   = retrieve_context(topic, q, k=6)\n",
    "    pred_ret  = qa(question=q, context=ctx_ret).get(\"answer\",\"\") if ctx_ret else \"\"\n",
    "\n",
    "    rows.append({\n",
    "        \"topic\": topic,\n",
    "        \"question\": q,\n",
    "        \"gold_answer\": gold,\n",
    "        \"pred_literal\": pred_lit,\n",
    "        \"pred_pragmatic\": pred_prag,\n",
    "        \"pred_retrieved\": pred_ret\n",
    "    })\n",
    "\n",
    "pred_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "386cea08b3dfeb92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T11:58:51.277006Z",
     "start_time": "2025-09-06T11:44:56.110526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 179 / 179 examples: 100%|██████████| 179/179 [04:32<00:00,  1.52s/it]\n",
      "Processed 179 / 179 examples: 100%|██████████| 179/179 [04:07<00:00,  1.38s/it]\n",
      "Processed 135 / 135 examples: 100%|██████████| 135/135 [05:15<00:00,  2.34s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     31\u001b[39m     f1   = \u001b[38;5;28mfloat\u001b[39m(np.mean([s.get(\u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m0.0\u001b[39m)        \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m scores])) \u001b[38;5;28;01mif\u001b[39;00m scores \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m:prec, \u001b[33m\"\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m\"\u001b[39m:rec, \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m:f1}\n\u001b[32m     34\u001b[39m summary = pd.DataFrame.from_records([\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mliteral\u001b[39m\u001b[33m\"\u001b[39m,   **\u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores_lit\u001b[49m\u001b[43m)\u001b[49m},\n\u001b[32m     36\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mpragmatic\u001b[39m\u001b[33m\"\u001b[39m, **summarize(scores_prag)},\n\u001b[32m     37\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mretrieved\u001b[39m\u001b[33m\"\u001b[39m, **summarize(scores_ret)},\n\u001b[32m     38\u001b[39m ])\n\u001b[32m     39\u001b[39m summary[\u001b[33m\"\u001b[39m\u001b[33mmetric\u001b[39m\u001b[33m\"\u001b[39m] = metric_used\n\u001b[32m     41\u001b[39m pred_df.to_csv(\u001b[33m\"\u001b[39m\u001b[33mtraditional_qa_results.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36msummarize\u001b[39m\u001b[34m(scores)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msummarize\u001b[39m(scores):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     prec = \u001b[38;5;28mfloat\u001b[39m(np.mean([\u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m0.0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m scores])) \u001b[38;5;28;01mif\u001b[39;00m scores \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m     30\u001b[39m     rec  = \u001b[38;5;28mfloat\u001b[39m(np.mean([s.get(\u001b[33m\"\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m0.0\u001b[39m)    \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m scores])) \u001b[38;5;28;01mif\u001b[39;00m scores \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m     31\u001b[39m     f1   = \u001b[38;5;28mfloat\u001b[39m(np.mean([s.get(\u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m0.0\u001b[39m)        \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m scores])) \u001b[38;5;28;01mif\u001b[39;00m scores \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'float' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from dspy.evaluate import SemanticF1\n",
    "    sf1 = SemanticF1(decompositional=True)\n",
    "\n",
    "    golds = pred_df[\"gold_answer\"].tolist()\n",
    "    qs    = pred_df[\"question\"].tolist()\n",
    "    batch_lit  = build_sf1_batch(pred_df[\"pred_literal\"].tolist(),   golds, qs)\n",
    "    batch_prag = build_sf1_batch(pred_df[\"pred_pragmatic\"].tolist(), golds, qs)\n",
    "    mask_ret   = pred_df[\"pred_retrieved\"].fillna(\"\").str.len() > 0\n",
    "    batch_ret  = build_sf1_batch(pred_df.loc[mask_ret, \"pred_retrieved\"].tolist(),\n",
    "                                 pred_df.loc[mask_ret, \"gold_answer\"].tolist(),\n",
    "                                 pred_df.loc[mask_ret, \"question\"].tolist())\n",
    "\n",
    "\n",
    "    scores_lit  = sf1.batch(batch_lit,  num_threads=8)\n",
    "    scores_prag = sf1.batch(batch_prag, num_threads=8)\n",
    "    scores_ret  = sf1.batch(batch_ret,  num_threads=8)\n",
    "\n",
    "    metric_used = \"SemanticF1\"\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    metric_used = \"LexicalF1\"\n",
    "    scores_lit  = [lexical_f1(p,g) for p,g in zip(pred_df[\"pred_literal\"],   pred_df[\"gold_answer\"])]\n",
    "    scores_prag = [lexical_f1(p,g) for p,g in zip(pred_df[\"pred_pragmatic\"], pred_df[\"gold_answer\"])]\n",
    "    scores_ret  = [lexical_f1(p,g) for p,g in zip(pred_df[\"pred_retrieved\"], pred_df[\"gold_answer\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d3bbcf6b39dfc8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:03:32.981794Z",
     "start_time": "2025-09-06T12:03:32.967503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>literal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.395880</td>\n",
       "      <td>SemanticF1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pragmatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.366978</td>\n",
       "      <td>SemanticF1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>retrieved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.095506</td>\n",
       "      <td>SemanticF1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      config  precision  recall        f1      metric\n",
       "0    literal        NaN     NaN  0.395880  SemanticF1\n",
       "1  pragmatic        NaN     NaN  0.366978  SemanticF1\n",
       "2  retrieved        NaN     NaN  0.095506  SemanticF1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def summarize_any(scores, metric_used: str):\n",
    "    if not scores:\n",
    "        return {\"precision\": np.nan, \"recall\": np.nan, \"f1\": 0.0}\n",
    "\n",
    "    first = scores[0]\n",
    "    # Case 1: LexicalF1 -> list of dicts with precision/recall/f1\n",
    "    if isinstance(first, dict):\n",
    "        prec = float(np.mean([s.get(\"precision\", 0.0) for s in scores]))\n",
    "        rec  = float(np.mean([s.get(\"recall\",    0.0) for s in scores]))\n",
    "        f1   = float(np.mean([s.get(\"f1\",        0.0) for s in scores]))\n",
    "        return {\"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "    # Case 2: SemanticF1 -> list of floats\n",
    "    f1 = float(np.mean([float(s) for s in scores]))\n",
    "    return {\"precision\": np.nan, \"recall\": np.nan, \"f1\": f1}\n",
    "\n",
    "summary = pd.DataFrame.from_records([\n",
    "    {\"config\": \"literal\",   **summarize_any(scores_lit,  metric_used)},\n",
    "    {\"config\": \"pragmatic\", **summarize_any(scores_prag, metric_used)},\n",
    "    {\"config\": \"retrieved\", **summarize_any(scores_ret,  metric_used)},\n",
    "])\n",
    "summary[\"metric\"] = metric_used\n",
    "\n",
    "pred_df.to_csv(\"traditional_qa_results.csv\", index=False)\n",
    "summary.to_csv(\"traditional_qa_summary.csv\", index=False)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73d30fab7f0e0771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:05:57.068959Z",
     "start_time": "2025-09-06T12:05:57.063349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " ['A Nightmare on Elm Street (2010 film)',\n",
       "  'Alexander Hamilton',\n",
       "  'The Wonderful Wizard of Oz (book)',\n",
       "  'Popeye'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = []\n",
    "for t in val_df[\"topic\"].unique():\n",
    "    if not any((SOURCES / t).glob(\"*.html\")):\n",
    "        missing.append(t)\n",
    "len(missing), missing[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11dd3375bffc0f",
   "metadata": {},
   "source": [
    "## Results (SemanticF1 ↑ is better)\n",
    "\n",
    "|    Config | SemanticF1 |\n",
    "| --------: | ---------: |\n",
    "|   Literal | **0.3959** |\n",
    "| Pragmatic | **0.3670** |\n",
    "| Retrieved | **0.0955** |\n",
    "\n",
    "## What this shows\n",
    "\n",
    "* **Literal > Pragmatic > Retrieved.** This is expected: extractive QA excels when the answer is present literally; **pragmatic** answers require inference beyond verbatim spans; **retrieved** is hardest because the model must first get the right evidence then extract a short span. This ordering aligns with the dataset’s goal of stressing pragmatic understanding.\n",
    "* **Absolute levels are reasonable** for an extractive baseline under SemanticF1 (a semantic, not lexical, match metric).\n",
    "\n",
    "## Why “Retrieved” lags (diagnosis)\n",
    "\n",
    "1. **Missing sources in our corpus.** We observed several topics with **no HTML files**, producing empty contexts (and thus empty answers) for those rows, which drags the average.\n",
    "2. **Long-context truncation.** The default QA pipeline splits long contexts with `max_seq_len≈384` and a doc stride; concatenating many passages raises the chance the gold span falls outside a processed window. Bumping `max_seq_len` (e.g., 512) and `max_answer_len` can help.\n",
    "3. **Retriever quality & ordering.** A **bi-encoder** retriever may surface relevant passages, but **re-ranking** with a **cross-encoder** (e.g., MS MARCO MiniLM-L6-v2) usually lifts top-positions and downstream QA quality. Hybrid pipelines that mix **BM25 + dense** retrieval before re-ranking are also standard practice.\n",
    "4. **Cosine vs. dot product.** If you use FAISS, L2-**normalize embeddings** and use **inner product** to effectively compute cosine similarity—this is the recommended pattern.\n",
    "\n",
    "## Takeaways (for the report)\n",
    "\n",
    "* The **extractive baseline** behaves as expected on PragmatiCQA: strong on literal spans, somewhat lower on pragmatic answers, and much lower when relying on retrieval.\n",
    "* **Actionable fixes** (optional but recommended to note):\n",
    "\n",
    "  * Fill the **missing topic folders** so retrieved isn’t penalized by empty contexts.\n",
    "  * Increase QA pipeline limits to handle longer concatenated contexts (`max_seq_len`, `max_answer_len`).\n",
    "  * Improve retrieval: **hybrid (BM25+dense)** + **cross-encoder re-ranking** before feeding to the QA model.\n",
    "  * Keep FAISS as **IP on normalized vectors** for cosine equivalence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
