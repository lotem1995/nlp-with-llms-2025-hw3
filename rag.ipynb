{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7aae3a9",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "Implement a base RAG module in DSPy. \n",
    "Given a question, retrieve the top-k documents in a list of HTML documents, then pass them as context to an LLM.\n",
    "\n",
    "Refer to https://dspy.ai/tutorials/rag/. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edec48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load an extremely efficient local model for retrieval\n",
    "model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cuda\")\n",
    "\n",
    "# Create an embedder using the model's encode method\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "\n",
    "# Traverse a directory and read html files - extract text from the html files\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_html_files(directory, sources_root=\"../PragmatiCQA-sources\"):\n",
    "    \"\"\"\n",
    "    Read .html files from a path. Accepts:\n",
    "      - absolute path\n",
    "      - relative path (from current working directory)\n",
    "      - topic name or folder name (will be resolved under sources_root)\n",
    "    Returns a list of document texts. If path not found, returns [].\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "\n",
    "    # Normalize and resolve path\n",
    "    if os.path.isabs(directory):\n",
    "        path = directory\n",
    "    else:\n",
    "        # prefer the provided relative path if it exists\n",
    "        if os.path.exists(directory):\n",
    "            path = directory\n",
    "        else:\n",
    "            # treat `directory` as a topic name under the default sources root\n",
    "            path = os.path.join(sources_root, directory)\n",
    "\n",
    "    path = os.path.abspath(path)\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        # not throwing here so notebook flow is uninterrupted; caller can handle empty list\n",
    "        print(f\"Directory not found: {path}\")\n",
    "        return texts\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".html\"):\n",
    "            file_path = os.path.join(path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    soup = BeautifulSoup(file, 'html.parser')\n",
    "                    texts.append(soup.get_text())\n",
    "            except Exception as e:\n",
    "                # skip problematic files but report minimal info\n",
    "                print(f\"Warning: could not read {file_path}: {e}\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f634c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 406 documents. Will encode them below.\n"
     ]
    }
   ],
   "source": [
    "# Provide either an absolute path, a relative path, or a topic folder name.\n",
    "# Examples:\n",
    "#   directory = '/full/path/to/PragmatiCQA-sources/The Legend of Zelda'\n",
    "#   directory = '../PragmatiCQA-sources/The Legend of Zelda'\n",
    "#   directory = 'The Legend of Zelda'           # treated as topic name under sources_root\n",
    "\n",
    "directory = \"../PragmatiCQA-sources/The Legend of Zelda\"\n",
    "corpus = read_html_files(directory)\n",
    "if not corpus:\n",
    "    print(f\"No documents loaded from: {os.path.abspath(directory)}\")\n",
    "else:\n",
    "    print(f\"Loaded {len(corpus)} documents. Will encode them below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0051d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the retriever\n",
    "max_characters = 10000  # for truncating >99th percentile of documents\n",
    "topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
    "\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b129af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "lm = dspy.LM('xai/grok-3-mini')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06d8027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = search(question).passages\n",
    "        return self.respond(context=context, question=question)\n",
    "    \n",
    "rag = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499af707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main plot of The Legend of Zelda revolves around a young hero named Link who must save the kingdom of Hyrule from the evil Ganon, the Prince of Darkness. Ganon steals the Triforce of Power and seeks to conquer Hyrule, prompting Princess Zelda to break the Triforce of Wisdom into eight fragments and hide them across the land. Zelda sends her nursemaid, Impa, to find a brave warrior to stop Ganon. Link embarks on a quest to collect the fragments, explore dungeons, battle enemies, and ultimately confront Ganon to rescue Princess Zelda and restore peace to Hyrule.\n"
     ]
    }
   ],
   "source": [
    "answer = rag(question=\"What is the main plot of The Legend of Zelda?\")  # Example query\n",
    "\n",
    "print(answer.response)  # Print the response from the RAG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98e19c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Legend of Zelda was first released in 1986.\n"
     ]
    }
   ],
   "source": [
    "q = 'What year did the Legend of Zelda come out?' \n",
    "\n",
    "print(rag(question=q).response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a45c579",
   "metadata": {},
   "source": [
    "Part 4.3 â€” Traditional QA baseline (start)\n",
    "\n",
    "Plan:\n",
    "- Use the existing retriever `search` to obtain retrieved passages for a question.\n",
    "- Use Hugging Face's 'distilbert-base-cased-distilled-squad' extractive QA pipeline to answer the question given:\n",
    "  1) Literal spans (from dataset),\n",
    "  2) Pragmatic spans (from dataset),\n",
    "  3) Retrieved context (from `search`).\n",
    "- Evaluate these three configurations with dspy.evaluate.SemanticF1 on the first question of each conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e91a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up evaluation pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 179 validation documents\n",
      "\n",
      "Evaluating first questions from validation set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de5c6e331ab4764994d0d0227268b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating QA approaches:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "# 4.3 Part 1: Traditional QA Baseline Implementation\n",
    "from transformers import pipeline\n",
    "import json, os\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import dspy\n",
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "def setup_qa_pipeline(device=None):\n",
    "    \"\"\"Initialize a robust QA pipeline.\n",
    "    Tries a list of reliable HF models, detects CUDA availability,\n",
    "    avoids float16 on CPU, and falls back gracefully to a CPU pipeline.\n",
    "    Accepts device as: 'cuda', 0, -1, or None.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline as _pipeline\n",
    "\n",
    "    # Reliable candidate models\n",
    "    model_candidates = [\n",
    "        'distilbert/distilbert-base-cased-distilled-squad',\n",
    "        'deepset/roberta-base-squad2'\n",
    "    ]\n",
    "\n",
    "    last_err = None\n",
    "    # Normalize device intent\n",
    "    want_cuda = False\n",
    "    if isinstance(device, str) and device.lower().startswith('cuda'):\n",
    "        want_cuda = True\n",
    "    elif isinstance(device, int) and device >= 0:\n",
    "        want_cuda = True\n",
    "\n",
    "    for model_name in model_candidates:\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "            # Only use GPU if available\n",
    "            use_cuda = want_cuda and torch.cuda.is_available()\n",
    "            if use_cuda:\n",
    "                model = AutoModelForQuestionAnswering.from_pretrained(model_name).half().to('cuda')\n",
    "                device_id = 0\n",
    "            else:\n",
    "                model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "                device_id = -1\n",
    "            model.eval()\n",
    "            return _pipeline('question-answering', model=model, tokenizer=tokenizer, device=device_id)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            # try next candidate\n",
    "            continue\n",
    "\n",
    "    # Final fallback: let transformers download and create a CPU pipeline from a known model id\n",
    "    try:\n",
    "        return _pipeline('question-answering', model=model_candidates[0], device=-1)\n",
    "    except Exception as e:\n",
    "        print('Error setting up QA pipeline: ', last_err)\n",
    "        print('Final fallback failed: ', e)\n",
    "        # Re-raise to make failure visible to the caller\n",
    "        raise\n",
    "\n",
    "def load_validation_data(filename=\"val.jsonl\", dataset_dir=\"../PragmatiCQA/data\"):\n",
    "    \"\"\"Load and filter validation data to first questions only.\"\"\"\n",
    "    try:\n",
    "        with open(os.path.join(dataset_dir, filename), 'r') as f:\n",
    "            return [json.loads(line) for line in f]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading validation data: {e}\")\n",
    "        return []\n",
    "\n",
    "def evaluate_qa_approaches(val_data, search, qa_pipeline, metric, max_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate three QA approaches on first questions:\n",
    "    1. Using literal spans\n",
    "    2. Using pragmatic spans\n",
    "    3. Using retrieved context\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    samples = val_data[:max_samples] if max_samples else val_data\n",
    "    \n",
    "    for doc in tqdm(samples, desc=\"Evaluating QA approaches\"):\n",
    "        if not doc.get(\"qas\"):\n",
    "            continue\n",
    "            \n",
    "        # Get first question data\n",
    "        qa = doc[\"qas\"][0]\n",
    "        question = qa[\"q\"]\n",
    "        gold = qa[\"a\"]\n",
    "        \n",
    "        # Setup contexts\n",
    "        literal_spans = [s[\"text\"] for s in qa[\"a_meta\"].get(\"literal_obj\", [])]\n",
    "        pragmatic_spans = [s[\"text\"] for s in qa[\"a_meta\"].get(\"pragmatic_obj\", [])]\n",
    "        literal_context = \" \".join(literal_spans).strip()\n",
    "        pragmatic_context = \" \".join(pragmatic_spans).strip()\n",
    "        \n",
    "        try:\n",
    "            retrieved_passages = search(question).passages\n",
    "            retrieved_context = \" \".join(retrieved_passages).strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Retrieval error for question: {question}\\nError: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Generate answers\n",
    "        def get_answer(context):\n",
    "            if not context:\n",
    "                return \"\"\n",
    "            try:\n",
    "                result = qa_pipeline(question=question, context=context)\n",
    "                return result[\"answer\"] if isinstance(result, dict) else \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"QA pipeline error: {e}\")\n",
    "                return \"\"\n",
    "\n",
    "        answers = {\n",
    "            \"literal\": get_answer(literal_context),\n",
    "            \"pragmatic\": get_answer(pragmatic_context),\n",
    "            \"retrieved\": get_answer(retrieved_context)\n",
    "        }\n",
    "\n",
    "        # Compute scores\n",
    "        gold_ex = dspy.Example(question=question, response=gold, \n",
    "                             inputs={\"context\": retrieved_context})\n",
    "        \n",
    "        scores = {}\n",
    "        for approach, answer in answers.items():\n",
    "            pred_ex = dspy.Example(question=question, response=answer,\n",
    "                                 inputs={\"context\": retrieved_context})\n",
    "            try:\n",
    "                scores[approach] = metric(gold_ex, pred_ex)\n",
    "            except Exception as e:\n",
    "                print(f\"Scoring error for {approach}: {e}\")\n",
    "                scores[approach] = 0.0\n",
    "\n",
    "        results.append({\n",
    "            \"topic\": doc.get(\"topic\", \"unknown\"),\n",
    "            \"question\": question,\n",
    "            \"gold\": gold,\n",
    "            **answers,\n",
    "            **scores\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Setup components\n",
    "print(\"Setting up evaluation pipeline...\")\n",
    "qa_pipeline = setup_qa_pipeline(device='cuda')\n",
    "metric = SemanticF1(decompositional=True)\n",
    "val_data = load_validation_data()\n",
    "num_samples = 100\n",
    "if not val_data:\n",
    "    print(\"No validation data loaded!\")\n",
    "else:\n",
    "    print(f\"Loaded {len(val_data)} validation documents\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(\"\\nEvaluating first questions from validation set...\")\n",
    "    results_df = evaluate_qa_approaches(val_data, search, qa_pipeline, metric,max_samples=num_samples)\n",
    "    \n",
    "    # Show aggregate results\n",
    "    print(\"\\nAggregate Results:\")\n",
    "    agg_scores = results_df[[\"literal\", \"pragmatic\", \"retrieved\"]].mean()\n",
    "    print(\"\\nMean scores across all questions:\")\n",
    "    print(agg_scores)\n",
    "    \n",
    "    # Show sample predictions\n",
    "    print(\"\\nSample Predictions (first 3):\")\n",
    "    for _, row in results_df.head(3).iterrows():\n",
    "        print(\"\\nQuestion:\", row[\"question\"])\n",
    "        print(\"Gold:\", row[\"gold\"][:200], \"...\" if len(row[\"gold\"]) > 200 else \"\")\n",
    "        print(f\"Literal ({row['literal']:.2f}):\", row[\"literal\"])\n",
    "        print(f\"Pragmatic ({row['pragmatic']:.2f}):\", row[\"pragmatic\"])\n",
    "        print(f\"Retrieved ({row['retrieved']:.2f}):\", row[\"retrieved\"])\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(\"traditional_qa_results.csv\", index=False)\n",
    "print(\"\\nFull results saved to traditional_qa_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-with-llms-2025-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
